# GMM Based Gait Library

## 推断问题

推断问题是指在某个命题下收集到一系列数据，需要根据数据推断命题的置信度。
$$
B(H)=P(H|Data)=\frac{P(Data|H)P(H)}{P(Data)}
$$
为了比较后验概率，需要计算数据在不同命题下的似然和命题的先验。

似然描述了数据对不同命题的倾斜程度，在某些命题下当前数据出现的可能性会更大，此时对命题的判断更加依赖于数据；反之，对命题的判断更依赖于先验信息。

推断问题往往使用以下方法：

**通过参数枚举法计算**

设置参数网格，每个格点对应的参数的可能性相等，通过枚举计算每个参数点对应的似然值和先验，从而给出置信度分布函数的数值表达。优点是完整全面，缺点是由于分母的evidence涉及到高维度积分所以计算量相当大。

**最大似然法计算**

最大似然法往往用于模型参数估计，在参数先验均等的条件下，只去计算最大似然值从而得到最优的参数。

对于单高斯分布等比较简单的问题可以直接使用解析式得到最优参数。

对于复杂的分布如联合高斯分布，则需要使用迭代算法获取最优参数。

**共轭先验法计算最大后验**

为了解决分母积分困难的问题，在选取当前似然对应的共轭先验时，会使得后验和先验同分布，从而可以从代数上快速得到后验概率的分布，在迭代的贝叶斯推断过程中可以有效节省积分花费的时间。

**拉普拉斯法近似计算后验概率**

同样是为了解决积分问题，在似然函数最大值附近，可以使用泰勒展开，从而得到一个近似的高斯分布，利用该高斯分布的计算简便性可以很方便地快速计算出分母，从而得到后验概率估计值。







## 非线性拟合的基本方法

当我们收集到N组数据，$X_N, t_N=\lbrace x^{(n)}, t_n\rbrace ^N_{n=1}$，x为I维数据，输出t是实数值

我们期望从数据中推断出一个非线性函数$y(x;w)$来拟合输入输出数据，同时预测下一步的输出，期中w是y的参数

最简单的情况是使用固定的基函数，w为不同基函数的权重

例如，选择RBF(radial basis functions)作为基函数
$$
y(x;w)=\sum_{h=1}^{H}\omega_h\phi_h(x)
\\
\phi_h(x) = exp[-\frac{(x-c_h)^2}{2r^2}]
$$
虽然y对x是非线性的，但是对w是线性的，因此我们称该模型为线性模型。

我们还可以使用可以改变形状的基函数，同时权重也进行调整
$$
y(x;w)=\sum_{h=1}^{H}\omega_h^{(2)}tanh(\sum^{I}_{i=1}\omega^{(1)}_{hi}x+\omega^{(1)}_{h0})+\omega^{(2)}_0
$$
该形式等价于一个神经网络

在选择好合适的模型参数后，我们需要通过最大后验概率来估计模型参数
$$
P(w|t_N,X_N)=\frac{P(t_N|w,X_N)P(w)}{P(t_N|X_N)}
$$
该问题可以通过Laplace方法进行优化或者蒙特卡洛方法搜索最大值。

在获得参数的后验分布后，通过全概率公式边际化参数w来进行下一步的预测
$$
P(t_{N+1}|t_N,X_{N+1})=\int P(t_{N+1}|w,x^{(N+1)})P(w|t_N,X_N)d^Hw
$$
实际计算时也常用蒙特卡洛方法来近似计算，随机选取R个参数$w^{(r)}$
$$
P(t_{N+1}|t_N,X_{N+1})=\frac{1}{R}\sum_{r=1}^{R}P(t_{N+1}|w^{(r)},x^{N+1})
$$
如果不对y定义模型参数就产生了另一种非参数化的方法，通过直接最小化误差得到一个y的序列，其中比较常用的是spline smoothing method
$$
M(y(x)) = 1/2\beta\sum_{n=1}^{N}(y(x_n)-t_n)^2+1/2\alpha\int [y^{(p)}(x)]^2 dx
$$
通过指定p的阶数可以得到指定阶数的y，例如p=2则得到的y是三次样条曲线

## 从高斯分布到高斯过程

对有限维度问题来说，高斯分布可以被写作
$$
P(x|\mu,\delta)=\frac{1}{\sqrt{2\pi}\delta}exp(-\frac{(x-\mu)^2}{2\delta^2}) = \frac{1}{Z}exp[(x-\mu)^TA(x-\mu)]
$$
其中Z是$exp((x-\mu)^TA(x-\mu))$在x上的积分，归一化从而保证总积分为1

一个函数可以看作一个无限维度的向量，每一个点(y(x),x)对应一个维度

高斯过程是无穷维高斯分布的推广，描述了y(x)在无穷维函数空间上的分布情况
$$
P(y(x)|\mu(x),A)=\frac{1}{Z}exp[-\frac{1}{2}(y(x)-\mu(x))^TA(y(x)-\mu(x))]
$$
此处的写法并不完全严谨，因为A并不是一个矩阵

此处需要先引入函数的内积和范数的关系
$$
||f||_2 = \sqrt{\int f(x)^2dx}
\\
f(x)^Tf(x)=\int f(x)^2dx
$$
当我们对y(x)求导，得到的是一个线性算子作用于y(x)
$$
y^p(x) = D^p(y(x))=D^py(x)
$$
则$-\frac{1}{2}(y(x)-\mu(x))^TA(y(x)-\mu(x))$实际上是导数的内积

当$\mu(x)=0$，在已知$y^p(x)=D^py(x)$的情况下高斯过程的指数形式可以写为
$$
lnP(y(x)|\alpha) = -\frac{1}{2}\alpha\int [D^py(x)]^2dx+const = -1/2y(x)^TAy(x)+const
$$
其中$A=[D^p]^TD^p$，其中虽然出现除了$\alpha$以外的参数，但需要知道$\alpha, D^{p}, A$指的是同一个条件，因此也常写作$ln P(y(x)|A)$，同时需要注意A由于内积的特性必须是正定的

从这种角度出发看无参数拟合问题可以看作是一种最大似然
$$
ln(y(x)|\alpha,t_N,\beta)=lnP(t_N|y(x),\beta)+lnP(y(x)|\alpha)
$$
